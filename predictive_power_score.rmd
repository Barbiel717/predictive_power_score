---
title: "Feature Importance: Predictive Power Score"
author: "registea"
date: "13/07/2020"
output: github_document
---

<center><img src="https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png"></center>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This notebook explores the Predictive Power Score (PPS) filter method created by Florian Wetschoreck and posted on [Medium](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598). The article describes the PPS as a data type agnostic normalised score of predictive power. The example in the article provided was written in python, this notebook implements the PPS in R, via a custom function.

To explore the PPS, the house price prediction [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from kaggle is used. This dataset is relatively large from a dimensional perspective but relatively small with regards to observations.

This notebook will not focus on the exploratory analysis or feature engineering steps in the model building process, but jump directly to evaluating variable importance using this metric. If you are interested in a full analysis of this dataset, then please follow this link to my kaggle [kernal](https://www.kaggle.com/ar89dsl/house-price-eda-predictive-power-score).

```{r package_load, warning = FALSE, message = FALSE, echo=FALSE}

# Modelling Framework
library(tidymodels) # Predictive Framework
library(caret)# Predictive Framework

# Visualisations and formatting
library(scales) # Number formats
library(knitr) # Table

# Data handling Packages
library(tidyverse) # Data handling/ Graphics
library(data.table) # Data handling

# Optimisation packages
library(ompr) # MILP wrapper
library(ROI) # Solver interface
library(ROI.plugin.lpsolve)
library(ompr.roi) # Link ROI and OMPR
```


# Data Loading

Using the fread function from data.table, the training and testing data will be stored into a single dataframe called df_model. The variable 'SalesPrice' is the target variable and as it does not exist in the testing data it will be set to NA. An additional flag variable will be created 'testflag' to distinguish between training and testing sets. Further to this all variable names will be coverted to lower case for personal preference.  


```{r data_load, warning = FALSE, message = FALSE}

# Load and combine training and testing data
df_model <- 
  rbind(
      # Load training data
      fread("C:/Users/Anthony/Documents/Git/Project Portfolio/predictive_power_score/train.csv") 
          %>% mutate(testflag = "train"), # Add flag variable
      
      # Load training data
      fread("C:/Users/Anthony/Documents/Git/Project Portfolio/predictive_power_score/test.csv") %>% 
          mutate(SalePrice = NA, # add SalePice variable
                 testflag = "test") # add flag variable 
      ) %>% 
  set_names(., tolower(names(.))) %>% # Convert all names to lower case
  select(-id) # Remove house id variable

```

# Feature Selection

A large feature set can have a significant impact on computation time and redundant features can have adverse effects on model performance. Some algorithms have feature selection/ dimensionality reduction included as part of the model development e.g. GLMNET. However, other methods such as multiple regression will need a helping hand. Feature selection can also aid in more advanced applications such as tree based methods. The PPS is a filter method,w

* Filter method: Predictive Power Score (PPS) - This will be applied during engineering and pre-processing steps to assess the predictive power of new variables. It will also be used to choose between correlated predictors in linear regression
* Global method: Genetic Algorithms (GA) - Applied to the overall model build and evaluation process for linear regression model to give it a fighting chance





