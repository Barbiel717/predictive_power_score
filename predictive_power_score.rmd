---
title: "Feature Importance: Predictive Power Score"
author: "registea"
date: "13/07/2020"
output: github_document
---

<center><img src="https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png"></center>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This notebook explores the Predictive Power Score (PPS) filter method created by Florian Wetschoreck and posted on [Medium](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598). The article describes the PPS as a data type agnostic normalised score of predictive power. The example in the article provided was written in python, this notebook implements the PPS in R, via a custom function.

To explore the PPS, the house price prediction [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from kaggle is used. This dataset is relatively large from a dimensional perspective but relatively small with regards to observations.

This notebook will not focus on the exploratory analysis or feature engineering steps in the model building process, but jump directly to evaluating variable importance using this metric. If you are interested in a full analysis of this dataset, then please follow this link to my kaggle [kernal](https://www.kaggle.com/ar89dsl/house-price-eda-predictive-power-score).

```{r package_load, warning = FALSE, message = FALSE, echo=FALSE}

# Modelling Framework
library(tidymodels) # Predictive Framework
library(caret)# Predictive Framework

# Visualisations and formatting
library(scales) # Number formats
library(knitr) # Table
library(e1071) # Stats

# Data handling Packages
library(tidyverse) # Data handling/ Graphics
library(data.table) # Data handling

# Optimisation packages
library(ompr) # MILP wrapper
library(ROI) # Solver interface
library(ROI.plugin.lpsolve)
library(ompr.roi) # Link ROI and OMPR
```

```{r data_load, warning = FALSE, message = FALSE, echo=FALSE}

# Load and combine training and testing data
df_model <- 
  rbind(
      # Load training data
      fread("C:/Users/Anthony/Documents/Git/Project Portfolio/predictive_power_score/train.csv") 
          %>% mutate(testflag = "train"), # Add flag variable
      
      # Load training data
      fread("C:/Users/Anthony/Documents/Git/Project Portfolio/predictive_power_score/test.csv") %>% 
          mutate(SalePrice = NA, # add SalePice variable
                 testflag = "test") # add flag variable 
      ) %>% 
  set_names(., tolower(names(.))) %>% # Convert all names to lower case
  select(-id) # Remove house id variable

```

# Exploring the target Variable 'saleprice'

The histogram shows the distribution of the 'saleprice' variable across all house sales. We can see that the majority of houses are around 150k in price, this is confirmed by calculating the median which sits at `r format(median(df_model$saleprice, na.rm = T)/ 1000, big.mark = ",")`k. The data has a long tail to the right, indicating that there are a small number of high priced houses. The skewness of house prices is `r round(skewness(df_model$saleprice, na.rm = T), 2)`, as this is above 1 it indicates that the data is highly positively skewed.

```{r target_var_plot1, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}

# Visualise the distribution of house price
df_model %>%
  ggplot(aes(x = saleprice)) +
  geom_histogram(fill = "blue") +
  scale_x_continuous(breaks= seq(0, 800000, by=50000), labels = scales::comma) +
  labs(y = "Houses") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90)) +
  ggtitle("Distribution of House Prices")

```

Applying a log transformation to the 'saleprice' variable makes the the data more symetrical and reduces the skew to `r round(skewness(log(df_model$saleprice), na.rm = T), 2)`. A skewness value between 0 and 0.5 indicates it is now minimally skewed. This can be visualised below in the historgram, in which the distribution appears to be a closer representation of a normal distribution. The log transformation has been explored here but will be applied in a later section.

```{r target_var_plot2, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}

# log transformed distribution
ggplot(df_model, aes(log(saleprice))) +
        geom_blank() +
        geom_histogram(aes(y = ..density..), fill = "blue") +
        stat_function(fun = dnorm, 
                      args = c(mean = mean(log(df_model$saleprice), na.rm = TRUE), 
                               sd = sd(log(df_model$saleprice), na.rm = TRUE)), 
                      col = "red",
                      size = 2) +
  scale_x_continuous(breaks= seq(10, 14, by=0.5)) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90)) +
  labs(x = "saleprice (log)") +
  ggtitle("Log transformed Distribution of House Prices")
```

```{r data_prep, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}

# Impute missing data
df_model <-
  df_model %>%
    mutate(poolqc = ifelse(is.na(poolqc), "ANA", poolqc),
           miscfeature = ifelse(is.na(miscfeature), "ANA", miscfeature),
           alley = ifelse(is.na(alley), "ANA", alley),
           fence = ifelse(is.na(fence), "ANA", fence),
           fireplacequ = ifelse(is.na(fireplacequ), "ANA", fireplacequ),
           garagefinish = ifelse(is.na(garagefinish), "ANA", garagefinish),
           garagequal = ifelse(is.na(garagequal), "ANA", garagequal),
           garagecond = ifelse(is.na(garagecond), "ANA", garagecond),
           garagetype = ifelse(is.na(garagetype), "ANA", garagetype),
           bsmtcond = ifelse(is.na(bsmtcond), "ANA", bsmtcond),
           bsmtexposure = ifelse(is.na(bsmtexposure), "ANA", bsmtexposure),
           bsmtqual = ifelse(is.na(bsmtqual), "ANA", bsmtqual),
           bsmtfintype2 = ifelse(is.na(bsmtfintype2), "ANA", bsmtfintype2),
           bsmtfintype1 = ifelse(is.na(bsmtfintype1), "ANA", bsmtfintype1))

# Fix some data issues
df_model <-
  df_model %>%
    mutate(yearremodadd = ifelse(yearremodadd > yrsold, yrsold, yearremodadd),
           garageyrblt = ifelse(garageyrblt == 2207, 2007, garageyrblt),
           exterior2nd = ifelse(exterior2nd == "Brk Cmn", "BrkComm", exterior2nd),
           exterior2nd = ifelse(exterior2nd == "Wd Shng", "Wd Sdng", exterior2nd))

# Replace the bounds for outliers
f_outlier <-
  function(variable, focus)
  {
    
    # Calculate IQR
    IQR <- 
      (quantile(variable, 0.75, na.rm = T) - 
       quantile(variable, 0.25, na.rm = T)
       ) * 1.5
    
    # Calculate UB or LB
    if(focus == "UB")
    {
      bound <- IQR + quantile(variable, 0.75, na.rm = T) # Calculate bound
      variable <- ifelse(variable > bound & variable != 0, bound, variable) # Truncate variable
    } else if (focus == "LB") {
      bound <- quantile(variable, 0.25, na.rm = T) - IQR # Calculate bound
      variable <- ifelse(variable < bound & variable != 0, bound, variable) # Truncate variable
    } else {
      bound <- IQR + quantile(variable, 0.75, na.rm = T) # defaults to upper bound
      variable <- ifelse(variable > bound & variable != 0, bound, variable)  # Truncate variable
    }
  }


# Create an object using the bestNormalize package
yeojohnson_obj <- 
  bestNormalize::yeojohnson(df_model %>%
               filter(testflag == "train") %>%
               select(saleprice) %>%
               unlist() %>%
               as.numeric(), 
             standardize = FALSE)

# Create function to switch between data scales
f_target_transform <-
  function(vector, yeojohnson_obj, invert_flag)
  {
    
    if(invert_flag == FALSE) 
    {
      ifelse(is.na(vector), NA, log(predict(yeojohnson_obj, newdata = vector)))

    } else {
      
      ifelse(is.na(vector), NA, predict(yeojohnson_obj, newdata = exp(vector), inverse = T))
    }
  }

# Apply transformation to target variable
df_model$saleprice <- f_target_transform(df_model$saleprice, yeojohnson_obj, FALSE)

# Apply series of pre-processing steps
df_model <-
  df_model %>%
    mutate(yrsold = factor(yrsold),
           mosold = factor(mosold),
           mssubclass = factor(mssubclass),
           overallcond = factor(overallcond),
           bsmtfinsf1 = f_outlier(variable = bsmtfinsf1, focus = "UB"),
           garagearea = f_outlier(variable = garagearea, focus = "UB"),
           grlivarea = f_outlier(variable = grlivarea, focus = "UB"),
           lotarea = f_outlier(variable = lotarea, focus = "UB"),
           lotfrontage = f_outlier(variable = lotfrontage, focus = "UB"),
           miscval = f_outlier(variable = miscval, focus = "UB")
           )

# Variables with skew
vc_skew <-
  sapply(df_model %>% 
                 filter(!is.na(saleprice)) %>%
                 select_if(is.numeric),
               function(x) skewness(x, na.rm = T)) %>%
          as.data.frame() %>%
          rownames_to_column() %>%
          rename(variable = rowname,
                 skew = ".") %>%
          filter(skew > 1 | skew < -1 & variable != "saleprice") %>%
          arrange(desc(skew)) %>%
          select(variable) %>%
          unlist() %>%
          as.character()

# Recipes pre-processing steps
df_model <-
  df_model %>%
    recipe(saleprice ~ .) %>%
    step_medianimpute(all_numeric(), -all_outcomes()) %>% # median impute numeric data
    step_modeimpute(all_nominal(), -all_outcomes()) %>% # mode impute nominal data
    step_YeoJohnson(vc_skew) %>%  # Increase symmetry
    step_normalize(all_numeric(), -all_outcomes()) %>% # centre & scale
    #step_dummy(all_nominal(), -all_outcomes()) %>% # Convert to dummy variables
    step_zv(all_predictors()) %>% # Remove zero variance
    prep() %>%
    bake(df_model)

# remove object
rm(vc_skew, f_outlier)


```

# Feature Selection via Feature Importance

A high dimensional dataset can be very useful for prediction, the numerous combinations of predictors can be utilised by a model to accurately predict a target of interest. There are also drawbacks of having a large feature set, these primarily are on computation time and resources. It some cases multiple predictors contain similar features or have no meaninful relationship with the target variable. In these cases, the addiitonal features can have adverse effects on model performance. There are a few different approaches to selecting features, one of the simplest is using a filter approach. This approach, measures the relationship between an individual predictor and the target variable. It is simple, because it is evaluated without reference to other predictors, which it may have a meaningful relationship with.

## Introducing Predictive Power Score (PPS)

A comprehensive breakdown of the PPS can be found [here](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598) where Florian Wetschoreck introduces the approach. It is delicately summarised as *"an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1"*. A score of 0 indicates the independent variable has no relationship with the target and a score of 1 indicates a perfect relationship of predictive power. The approach uses a naive model and a evaluation metric (RMSE in this case), the naive model sits as the upper bound (worst case possible) and the individual predictor is evaluated with reference to how well it predicts in relation to the naive model.

The function 'f_PPS' built below, is the implementation of the PPS score and is summarised in the following way:

* Creates a two variable dataframe containing a single predictor and target variable
* Pre-process numerical data with normalisation or creating dummy variables for nominal data
* Builds a Cross Validated (CV) hyperparameter tuned decision tree and predicts on validation data
* Creates a naive model to be stored as the upper bound, in this case using the just the median 'saleprice'
* Compare decision tree evaluation metric with naive model in a normalisation function (.pred - UB) / (LB - UB) to restrict the score between 0 and 1

```{r f_pps, warning = FALSE, message = FALSE}

# Predictive Power Score Function
f_PPS <-
  function(predictor, target, model_mode,
           evaluation_metric, LB,
           grid_size, folds,
           seedval = 1989)
  {

    # Combine predictor and target variable from inputs
    df <-
      data.frame(predictor = predictor,
                 target = target
                 )

    # Add pre-processing steps
    recipe <-
      df %>%
        recipe(target ~ .) %>%
        step_dummy(all_nominal(), -all_outcomes()) %>%
        prep()

    # Apply transformation
    df <- recipe %>% bake(df)

    # Cross validation
    set.seed(seedval) # set the seed
    l_cv <- vfold_cv(df, v = folds, strata = "target") # Cross validation

    # Set the model engine
    mod <-
      decision_tree(mode = model_mode,
                    cost_complexity = tune(),
                    tree_depth = tune(),
                    min_n = tune()
                    ) %>%
      set_engine("rpart")

    # Evaluation metric
    if(evaluation_metric == "rmse") {
      metric_eval <- "rmse"
      metric_focus <- yardstick::metric_set(rmse)
    } else if (evaluation_metric == "mae") {
      metric_eval <- "mae"
      metric_focus <- yardstick::metric_set(mae)
    } else if (evaluation_metric == "mape") {
      metric_eval <- "mape"
      metric_focus <- yardstick::metric_set(mape)
    } else {
      metric_eval <- "rmse" # default
      metric_focus <- yardstick::metric_set(rmse)
    }

    # Hyperparameter tuning and store parameters
    set.seed(seedval) # set the seed
    df_parameter <-
      tune_grid(
                formula   = target ~ .,
                model     = mod,
                resamples = l_cv,
                grid      = grid_random(parameters(cost_complexity(),
                                                   tree_depth(),
                                                   min_n()),
                                        size = grid_size),
                metrics   = metric_focus,
                control   = control_grid(verbose = FALSE)
          ) %>%
      select_best(metric_eval)


    # CV Optimised Model
    mod_final <-
      list(parameters = df_parameter,
           df = map2_df(.x = l_cv$splits,
                        .y = l_cv$id,
                        function (split = .x, fold = .y)
                         {
                           # Split the data into analysis and assessment tables
                           df_analysis <- analysis(split)
                           df_assessment <- assessment(split)

                           # Build the model
                           mod_2 <-
                            decision_tree(mode = model_mode,
                                          cost_complexity = as.numeric(df_parameter["cost_complexity"]),
                                          tree_depth = as.numeric(df_parameter["tree_depth"]),
                                          min_n = as.numeric(df_parameter["min_n"])
                                         ) %>%
                             set_engine("rpart") %>%
                             fit(target ~ ., data = df_analysis)

                           # Summarise Predictions
                           table <-
                             tibble(fold = fold,
                                    truth = df_assessment$target,
                                    .pred = predict(mod_2, new_data = df_assessment) %>% unlist()
                                    )
                            })
              )

    # Predict using the model and naive approach
    df_output <- # upper bound alignment
        data.frame(truth = target, # actual house price
                   .naive = median(target)
                   )
    

    # Calculate the upper bound and predictive score
    if(evaluation_metric == "rmse") { 
      
      # Calculate rmse for naive model
      UB <-
        df_output %>%
        rmse(truth, .naive) %>%
        select(.estimate) %>%
        as.numeric()
      
      # Calculate rmse for tuned decision tree
      pred <-
        mod_final[["df"]] %>%
        rmse(truth, .pred) %>%
        select(.estimate) %>%
        as.numeric() 
      
    } else if (evaluation_metric == "mae") {
      
      # Calculate mae for naive model
      UB <-
        df_output %>%
        mae(truth, .naive) %>%
        select(.estimate) %>%
        as.numeric()
      
      # Calculate mae for tuned decision tree
      pred <-
        mod_final[["df"]] %>%
        mae(truth, .pred) %>%
        select(.estimate) %>%
        as.numeric()     
      
    } else if (evaluation_metric == "mape") {
      
      # Calculate mape for naive model
      UB <-
        df_output %>%
        mape(truth, .naive) %>%
        select(.estimate) %>%
        as.numeric()
      
      # Calculate mape for tuned decision tree
      pred <-
        mod_final[["df"]] %>%
        mape(truth, .pred) %>%
        select(.estimate) %>%
        as.numeric() 
 
    } else {
      
      # Calculate rmse for naive model
      UB <-
        df_output %>%
        rmse(truth, .naive) %>%
        select(.estimate) %>%
        as.numeric()
      
      # Calculate rmse for tuned decision tree
      pred <-
        mod_final[["df"]] %>%
        rmse(truth, .pred) %>%
        select(.estimate) %>%
        as.numeric()  
    }

    # Calculate PPS score
    PPS <- (pred - UB) / (LB - UB)
    
    # Return PPS score
    return(PPS)

    }

```

Now that we have a PPS function set-up, the output can be tested on a couple of variables. A quick visual correlation analysis is run on the numerical variables to choose some variables which exhibit strong and weak linear relationships.

```{r corr_plot, echo = FALSE, warning = FALSE, message = FALSE, fig.align="center", fig.height=12, fig.width=12}

# Distribution of house prices
cbind(
      df_model %>%
        select_if(is.numeric) %>%
        select(-c(saleprice)),
      df_model %>%
        mutate(saleprice = log(saleprice)) %>%
        select(saleprice)
      ) %>%
  gather(-saleprice, key = "var", value = "value")  %>% 
  ggplot(aes(x = value, y = saleprice, col = saleprice)) +
    geom_jitter() +
    theme(axis.text.x = element_text(angle = 90),
          plot.title = element_text(hjust = 0.5,
                                    size = 20)) + 
    geom_smooth(method = "lm") +
    facet_wrap(~ var, scales = "free") +
    ggtitle("Relationship with numeric predictors and log of 'saleprice'")

```

The following two variables are chosen:

* overallqual: Numerical variable which plot indicates strong positive correlation
* miscval: Numerical variable which the plot indicates near zero variance

The table below shows that none of the variables in their own right have a particularly high PPS score. However, the variable indentified as being important (overallqual) seem to have a relatively high score, while miscval is virtually 0.

```{r test_pps, echo = FALSE, warning = FALSE, message = FALSE}

# Isolate saleprice in training data  
df_target <- 
  df_model %>% 
  filter(!is.na(saleprice)) %>% 
  mutate(saleprice = log(saleprice)) %>%
  select(saleprice)


# Calculate PPS
sapply(df_model %>% 
         filter(!is.na(saleprice)) %>%
         select(overallqual, 
                bsmtfinsf2)
         ,
       function(var)
         f_PPS(predictor = var, # predictor variable
               target = df_target$saleprice, # target variable
               model_mode = "regression", # regression or classification
               evaluation_metric = "rmse", # prediction evaluation metrics
               LB = 0, # Lower Bound e.g. RMSE perfect score is 0
               grid_size = 5, # Size of tuning grid
               folds = 10 # Folds in cross validation
               )
       ) %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  rename(Variable = rowname,
         PPS = ".") %>%
  mutate(PPS = percent(round(PPS,3))) %>%
  kable(align = c('c', 'c'))  

# Remove PPS
rm(df_target)

```

# Model Building

Now that we have a PPS function, each predictor will be scored using it's methodology. The predictors with the top scores will be retained and entered into a linear regression model.

```{r full_pps, echo = FALSE, warning = FALSE, message = FALSE}
system.time({
  
  # Isolate saleprice in training data  
  df_target <- 
    df_model %>% 
    filter(!is.na(saleprice)) %>% 
    mutate(saleprice = log(saleprice)) %>%
    select(saleprice)
  
  # Calculate PPS
  df_pps_full <-
  sapply(df_model %>% 
           filter(!is.na(saleprice)) %>%
           select(-saleprice)
           ,
         function(var)
           f_PPS(predictor = var, # predictor variable
                 target = df_target$saleprice, # target variable
                 model_mode = "regression", # regression or classification
                 evaluation_metric = "rmse", # prediction evaluation metrics
                 LB = 0, # Lower Bound e.g. RMSE perfect score is 0
                 grid_size = 5, # Size of tuning grid
                 folds = 5 # Folds in cross validation
                 )
         ) %>%
    as.data.frame() %>%
    rownames_to_column() %>%
    rename(Variable = rowname,
           PPS = ".")
  
  # Remove PPS
  rm(df_target)
})
```


```{r full_pps, echo = FALSE, warning = FALSE, message = FALSE}

# visualise distirbution of pps
df_pps_full %>%
  ggplot(aes(x = "PPS", y = PPS)) +
    geom_boxplot(fill = "red")  +
    theme(axis.text.x = element_text(angle = 90),
          plot.title = element_text(hjust = 0.5,
                                    size = 20)) + 
    ggtitle("Predictor Distribution of PPS")




```

