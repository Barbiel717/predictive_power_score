---
title: "Feature Importance: Predictive Power Score"
author: "registea"
date: "13/07/2020"
output: github_document
---

<center><img src="https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png"></center>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This notebook explores the Predictive Power Score (PPS) filter method created by Florian Wetschoreck and posted on [Medium](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598). The article describes the PPS as a data type agnostic normalised score of predictive power. The example in the article provided was written in python, this notebook implements the PPS in R, via a custom function.

To explore the PPS, the house price prediction [dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from kaggle is used. This dataset is relatively large from a dimensional perspective but relatively small with regards to observations.

This notebook will not focus on the exploratory analysis or feature engineering steps in the model building process, but jump directly to evaluating variable importance using this metric. If you are interested in a full analysis of this dataset, then please follow this link to my kaggle [kernal](https://www.kaggle.com/ar89dsl/house-price-eda-predictive-power-score).

```{r package_load, warning = FALSE, message = FALSE, echo=FALSE}

# Modelling Framework
library(tidymodels) # Predictive Framework
library(caret)# Predictive Framework

# Visualisations and formatting
library(scales) # Number formats
library(knitr) # Table
library(e1071) # Stats

# Data handling Packages
library(tidyverse) # Data handling/ Graphics
library(data.table) # Data handling

# Optimisation packages
library(ompr) # MILP wrapper
library(ROI) # Solver interface
library(ROI.plugin.lpsolve)
library(ompr.roi) # Link ROI and OMPR
```

```{r data_load, warning = FALSE, message = FALSE, echo=FALSE}

# Load and combine training and testing data
df_model <- 
  rbind(
      # Load training data
      fread("C:/Users/Anthony/Documents/Git/Project Portfolio/predictive_power_score/train.csv") 
          %>% mutate(testflag = "train"), # Add flag variable
      
      # Load training data
      fread("C:/Users/Anthony/Documents/Git/Project Portfolio/predictive_power_score/test.csv") %>% 
          mutate(SalePrice = NA, # add SalePice variable
                 testflag = "test") # add flag variable 
      ) %>% 
  set_names(., tolower(names(.))) %>% # Convert all names to lower case
  select(-id) # Remove house id variable

```

# Exploring the target Variable 'saleprice'

The histogram shows the distribution of the 'saleprice' variable across all house sales. We can see that the majority of houses are around 150k in price, this is confirmed by calculating the median which sits at `r format(median(df_model$saleprice, na.rm = T)/ 1000, big.mark = ",")`k. The data has a long tail to the right, indicating that there are a small number of high priced houses. The skewness of house prices is `r round(skewness(df_model$saleprice, na.rm = T), 2)`, as this is above 1 it indicates that the data is highly positively skewed.

```{r target_var_plot1 echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}

# Visualise the distribution of house price
df_model %>%
  ggplot(aes(x = saleprice)) +
  geom_histogram(fill = "blue") +
  scale_x_continuous(breaks= seq(0, 800000, by=50000), labels = scales::comma) +
  labs(y = "Houses") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90)) +
  ggtitle("Distribution of House Prices")

```

Applying a log transformation to the 'saleprice' variable makes the the data more symetrical and reduces the skew to `r round(skewness(log(df_model$saleprice), na.rm = T), 2)`. A skewness value between 0 and 0.5 indicates it is now minimally skewed. This can be visualised below in the historgram, in which the distribution appears to be a closer representation of a normal distribution. The log transformation has been explored here but will be applied in a later section.

```{r target_var_plot2 echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}

# log transformed distribution
ggplot(df_model, aes(log(saleprice))) +
        geom_blank() +
        geom_histogram(aes(y = ..density..), fill = "blue") +
        stat_function(fun = dnorm, 
                      args = c(mean = mean(log(df_model$saleprice), na.rm = TRUE), 
                               sd = sd(log(df_model$saleprice), na.rm = TRUE)), 
                      col = "red",
                      size = 2) +
  scale_x_continuous(breaks= seq(10, 14, by=0.5)) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90)) +
  labs(x = "saleprice (log)") +
  ggtitle("Log transformed Distribution of House Prices")
```


```{r data_prep echo = FALSE, warning = FALSE, message = FALSE, fig.align="center"}

# log transformed distribution
ggplot(df_model, aes(log(saleprice))) +
        geom_blank() +
        geom_histogram(aes(y = ..density..), fill = "blue") +
        stat_function(fun = dnorm, 
                      args = c(mean = mean(log(df_model$saleprice), na.rm = TRUE), 
                               sd = sd(log(df_model$saleprice), na.rm = TRUE)), 
                      col = "red",
                      size = 2) +
  scale_x_continuous(breaks= seq(10, 14, by=0.5)) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90)) +
  labs(x = "saleprice (log)") +
  ggtitle("Log transformed Distribution of House Prices")
```

# Feature Selection via Feature Importance

A high dimensional dataset can be very useful for prediction, the numerous combinations of predictors can be utilised by a model to accurately predict a target of interest. There are also drawbacks of having a large feature set, these primarily are on computation time and resources. It some cases multiple predictors contain similar features or have no meaninful relationship with the target variable. In these cases, the addiitonal features can have adverse effects on model performance. There are a few different approaches to selecting features, one of the simplest is using a filter approach. This approach, measures the relationship between an individual predictor and the target variable. It is simple, because it is evaluated without reference to other predictors, which it may have a meaningful relationship with.




